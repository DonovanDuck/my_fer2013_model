{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 自己调试网络结构定义和训练过程\n",
    "网络是lenet-5改\n",
    "其实更像照着alexnet改的，如果是alexnet，norm全在pooling之后,lrn和BN确实不同，lrn是active和pooling之后\n",
    "\n",
    "\n",
    "tf.bias_add、tf.matmul、tf.multiply都是可以重名的，如果不自定义名称，这些会形成一个数组，导致图形不连贯\n",
    "\n",
    "todo:ema的IsVariableInitialized[0-11],也是重名数组，但是ema一般都是简单调用的两个操作，这一步被隐藏了，这个对整体结构图影响不大，应该是一个原地闭路，现在打印成开路了。可以手动拆，ungroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 反向传播\n",
    "fer_forward定义在同一个笔记下，不import了。直接调用forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "NUM_CHANNELS = 1\n",
    "# CONV1_KSIZE = 5\n",
    "# CONV1_OUTPUT_CHANNEL = 64\n",
    "# CONV2_KSIZE = 5\n",
    "# CONV2_OUTPUT_CHANNEL = 64\n",
    "# CONV3_KSIZE = 5\n",
    "# CONV3_OUTPUT_CHANNEL = 128\n",
    "\n",
    "FC_SIZE = 512\n",
    "OUTPUT_NODE = 7\n",
    "import fer_config as config\n",
    "import fer_generator\n",
    "import fer_forward\n",
    "import time\n",
    "BATCH_SIZE = 4#128\n",
    "LEARNING_RATE_BASE = 0.0005\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "STEPS = 5\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "train_num_examples = 28709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward():\n",
    "    global_step = tf.Variable(0, trainable = False)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        train_num_examples / BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    x = tf.placeholder(tf.float32, [BATCH_SIZE, config.img_width, config.img_height, NUM_CHANNELS])\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE])\n",
    "    \n",
    "    y = fer_forward.forward(x,True)\n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))\n",
    "    cem = tf.reduce_mean(ce,name='cross_entropy_reduce_mean')\n",
    "    total_loss = tf.add(cem, tf.add_n(tf.get_collection('regularization_losses')), name='total_loss')\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss, global_step = global_step)\n",
    "    \n",
    "    #打印graph的时候把ema屏蔽掉，不然有一些ema的初始化会把图弄的很乱\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step,name='ema')\n",
    "    print('ema:',ema)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    print('ema_op:',ema_op)\n",
    "    \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)#BN需要依赖操作，但是为什么空？即使没有ema也是空。\n",
    "    print(tf.global_variables())\n",
    "    print('tf.GraphKeys.UPDATE_OPS：',tf.GraphKeys.UPDATE_OPS)\n",
    "    print('update_ops:',update_ops)\n",
    "    \n",
    "    print('type of ema_op:',type(ema_op))\n",
    "    print('type of update_ops:',type(update_ops))\n",
    "    with tf.control_dependencies([train_step, ema_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name='accuracy')#先转换类型再计算\n",
    "    \n",
    "    \n",
    "\n",
    "    img_batch, label_batch = fer_generator.get_tfrecord(BATCH_SIZE, config.tfRecord_train)#位置没影响\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.summary.FileWriter(logdir='my_net_graph', graph=sess.graph)#存网络\n",
    "\n",
    "\n",
    "        print('will start!')\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "#         img_batch, label_batch = get_tfrecord(BATCH_SIZE, config.tfRecord_train, dbg = True)\n",
    "        print(type(img_batch))\n",
    "        for i in range(STEPS):#调试完，替换回STEPS\n",
    "#             print(' in for loop')\n",
    "            xs, ys = sess.run([img_batch, label_batch])\n",
    "#             print(type(xs))\n",
    "#             print(type(ys))\n",
    "#             print('ys:',ys)\n",
    "            reshaped_xs = np.reshape(xs, (BATCH_SIZE,config.img_width,config.img_height,NUM_CHANNELS))\n",
    "            #数据出了点问题，train那个tfrecord0字节，可能有覆写\n",
    "            _, loss_value, accuracy_value, step = sess.run([train_op, total_loss, accuracy, global_step],\n",
    "                                                          feed_dict={x:reshaped_xs, y_:ys})\n",
    "            print('loss:',loss_value)\n",
    "            if (i + 1) % 2 == 0:\n",
    "                print('%d'%(step))\n",
    "                print('%g'%(loss_value))\n",
    "                print('%g'%(accuracy_value))\n",
    "                print(\"%s : After %d training step(s), loss,accuracy on training batch is %g , %g.\" % (time.strftime('%Y-%m-%d %H:%M:%S'), step, loss_value,accuracy_value))\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ema: <tensorflow.python.training.moving_averages.ExponentialMovingAverage object at 0x7f7e2824ccf8>\n",
      "ema_op: name: \"ema\"\n",
      "op: \"NoOp\"\n",
      "input: \"^ema/AssignMovingAvg\"\n",
      "input: \"^ema/AssignMovingAvg_1\"\n",
      "input: \"^ema/AssignMovingAvg_2\"\n",
      "input: \"^ema/AssignMovingAvg_3\"\n",
      "input: \"^ema/AssignMovingAvg_4\"\n",
      "input: \"^ema/AssignMovingAvg_5\"\n",
      "input: \"^ema/AssignMovingAvg_6\"\n",
      "input: \"^ema/AssignMovingAvg_7\"\n",
      "input: \"^ema/AssignMovingAvg_8\"\n",
      "input: \"^ema/AssignMovingAvg_9\"\n",
      "input: \"^ema/AssignMovingAvg_10\"\n",
      "input: \"^ema/AssignMovingAvg_11\"\n",
      "input: \"^ema/AssignMovingAvg_12\"\n",
      "input: \"^ema/AssignMovingAvg_13\"\n",
      "input: \"^ema/AssignMovingAvg_14\"\n",
      "input: \"^ema/AssignMovingAvg_15\"\n",
      "input: \"^ema/AssignMovingAvg_16\"\n",
      "input: \"^ema/AssignMovingAvg_17\"\n",
      "\n",
      "[<tf.Variable 'Variable:0' shape=() dtype=int32_ref>, <tf.Variable 'conv1_w:0' shape=(5, 5, 1, 64) dtype=float32_ref>, <tf.Variable 'conv1_b:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/moving_mean:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/moving_variance:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv2_w:0' shape=(5, 5, 64, 64) dtype=float32_ref>, <tf.Variable 'conv2_b:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/moving_mean:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/moving_variance:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv3_w:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'conv3_b:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'fc1_w:0' shape=(4608, 512) dtype=float32_ref>, <tf.Variable 'fc2_b:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc2_w:0' shape=(512, 512) dtype=float32_ref>, <tf.Variable 'fc2_b_1:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc3_w:0' shape=(512, 7) dtype=float32_ref>, <tf.Variable 'fc3_b:0' shape=(7,) dtype=float32_ref>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'conv1_w/Adam:0' shape=(5, 5, 1, 64) dtype=float32_ref>, <tf.Variable 'conv1_w/Adam_1:0' shape=(5, 5, 1, 64) dtype=float32_ref>, <tf.Variable 'conv1_b/Adam:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv1_b/Adam_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/gamma/Adam:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/gamma/Adam_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/beta/Adam:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/beta/Adam_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv2_w/Adam:0' shape=(5, 5, 64, 64) dtype=float32_ref>, <tf.Variable 'conv2_w/Adam_1:0' shape=(5, 5, 64, 64) dtype=float32_ref>, <tf.Variable 'conv2_b/Adam:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv2_b/Adam_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/gamma/Adam:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/gamma/Adam_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/beta/Adam:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/beta/Adam_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv3_w/Adam:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'conv3_w/Adam_1:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'conv3_b/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'conv3_b/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/gamma/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/gamma/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/beta/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/beta/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'fc1_w/Adam:0' shape=(4608, 512) dtype=float32_ref>, <tf.Variable 'fc1_w/Adam_1:0' shape=(4608, 512) dtype=float32_ref>, <tf.Variable 'fc2_b/Adam:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc2_b/Adam_1:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc2_w/Adam:0' shape=(512, 512) dtype=float32_ref>, <tf.Variable 'fc2_w/Adam_1:0' shape=(512, 512) dtype=float32_ref>, <tf.Variable 'fc2_b_1/Adam:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc2_b_1/Adam_1:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc3_w/Adam:0' shape=(512, 7) dtype=float32_ref>, <tf.Variable 'fc3_w/Adam_1:0' shape=(512, 7) dtype=float32_ref>, <tf.Variable 'fc3_b/Adam:0' shape=(7,) dtype=float32_ref>, <tf.Variable 'fc3_b/Adam_1:0' shape=(7,) dtype=float32_ref>, <tf.Variable 'conv1_w/ema:0' shape=(5, 5, 1, 64) dtype=float32_ref>, <tf.Variable 'conv1_b/ema:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/gamma/ema:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/beta/ema:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv2_w/ema:0' shape=(5, 5, 64, 64) dtype=float32_ref>, <tf.Variable 'conv2_b/ema:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/gamma/ema:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/beta/ema:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv3_w/ema:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'conv3_b/ema:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/gamma/ema:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/beta/ema:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'fc1_w/ema:0' shape=(4608, 512) dtype=float32_ref>, <tf.Variable 'fc2_b/ema:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc2_w/ema:0' shape=(512, 512) dtype=float32_ref>, <tf.Variable 'fc2_b_1/ema:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc3_w/ema:0' shape=(512, 7) dtype=float32_ref>, <tf.Variable 'fc3_b/ema:0' shape=(7,) dtype=float32_ref>]\n",
      "tf.GraphKeys.UPDATE_OPS： update_ops\n",
      "update_ops: [<tf.Tensor 'bn1/AssignMovingAvg:0' shape=(64,) dtype=float32_ref>, <tf.Tensor 'bn1/AssignMovingAvg_1:0' shape=(64,) dtype=float32_ref>, <tf.Tensor 'bn2/AssignMovingAvg:0' shape=(64,) dtype=float32_ref>, <tf.Tensor 'bn2/AssignMovingAvg_1:0' shape=(64,) dtype=float32_ref>, <tf.Tensor 'bn3/AssignMovingAvg:0' shape=(128,) dtype=float32_ref>, <tf.Tensor 'bn3/AssignMovingAvg_1:0' shape=(128,) dtype=float32_ref>]\n",
      "type of ema_op: <class 'tensorflow.python.framework.ops.Operation'>\n",
      "type of update_ops: <class 'list'>\n",
      "filename_queue: <tensorflow.python.ops.data_flow_ops.FIFOQueue object at 0x7f7e27c3ccf8>\n",
      "will start!\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "loss: 8.52077\n",
      "loss: 8.509\n",
      "2\n",
      "8.509\n",
      "0\n",
      "2019-03-01 23:29:48 : After 2 training step(s), loss,accuracy on training batch is 8.509 , 0.\n",
      "loss: 8.14398\n",
      "loss: 8.31882\n",
      "4\n",
      "8.31882\n",
      "0\n",
      "2019-03-01 23:29:48 : After 4 training step(s), loss,accuracy on training batch is 8.31882 , 0.\n",
      "loss: 8.3215\n"
     ]
    }
   ],
   "source": [
    "backward()\n",
    "\n",
    "#EMA有18变量，BN：3*2=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取tfRecord文件\n",
    "def read_tfRecord(tfRecord_path):\n",
    "    filename_queue = tf.train.string_input_producer([tfRecord_path])\n",
    "    print('filename_queue:',filename_queue)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _,serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={\n",
    "                                           'label':tf.FixedLenFeature([7],tf.int64),\n",
    "                                           'img_raw':tf.FixedLenFeature([],tf.string)\n",
    "                                                 })\n",
    "    img = tf.decode_raw(features['img_raw'],tf.uint8)\n",
    "    img.set_shape([config.img_height * config.img_height])\n",
    "\n",
    "    img = tf.cast(img,tf.float32)*(1./255)\n",
    "    label = tf.cast(features['label'],tf.float32)\n",
    "    return img,label\n",
    "\n",
    "\n",
    "# 批量读取数据\n",
    "def get_tfrecord(num, tfRecord_path,dbg = False):\n",
    "    img,label=read_tfRecord(tfRecord_path)\n",
    "    print('img readed:',img)\n",
    "    print('label readed:',label)\n",
    "    if dbg:#如果调试，不走shuffle\n",
    "        return img, label\n",
    "    img_batch,label_batch = tf.train.shuffle_batch([img,label],\n",
    "                                                   batch_size =num,\n",
    "                                                   num_threads=2,\n",
    "                                                   capacity=10000,\n",
    "                                                   min_after_dequeue=5000)\n",
    "    print('return img_batch:',img_batch)\n",
    "    print('return label_batch:',label_batch)\n",
    "    return img_batch,label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function decode_raw in module tensorflow.python.ops.gen_parsing_ops:\n",
      "\n",
      "decode_raw(bytes, out_type, little_endian=True, name=None)\n",
      "    Reinterpret the bytes of a string as a vector of numbers.\n",
      "    \n",
      "    Args:\n",
      "      bytes: A `Tensor` of type `string`.\n",
      "        All the elements must have the same length.\n",
      "      out_type: A `tf.DType` from: `tf.half, tf.float32, tf.float64, tf.int32, tf.uint16, tf.uint8, tf.int16, tf.int8, tf.int64`.\n",
      "      little_endian: An optional `bool`. Defaults to `True`.\n",
      "        Whether the input `bytes` are in little-endian order.\n",
      "        Ignored for `out_type` values that are stored in a single byte like\n",
      "        `uint8`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` of type `out_type`.\n",
      "      A Tensor with one more dimension than the input `bytes`.  The\n",
      "      added dimension will have size equal to the length of the elements\n",
      "      of `bytes` divided by the number of bytes to represent `out_type`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# help(tf.decode_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 专门调一下tfrecord的读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will start!\n",
      "filename_queue: <tensorflow.python.ops.data_flow_ops.FIFOQueue object at 0x7fd99c4a7eb8>\n",
      "serialized_example: Tensor(\"ReaderReadV2_1:1\", shape=(), dtype=string)\n",
      "features: {'img_raw': <tf.Tensor 'ParseSingleExample_1/Squeeze_img_raw:0' shape=() dtype=string>, 'label': <tf.Tensor 'ParseSingleExample_1/Squeeze_label:0' shape=(7,) dtype=int64>}\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#     img_batch, label_batch = fer_generator.get_tfrecord(BATCH_SIZE, config.tfRecord_train)#位置没影响\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    print('will start!')\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    ####################################################\n",
    "    filename_queue = tf.train.string_input_producer([config.tfRecord_train])\n",
    "    print('filename_queue:',filename_queue)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _,serialized_example = reader.read(filename_queue)\n",
    "    print('serialized_example:',serialized_example)\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={\n",
    "                                           'label':tf.FixedLenFeature([7],tf.int64),\n",
    "                                           'img_raw':tf.FixedLenFeature([],tf.string)\n",
    "                                                 })\n",
    "    print('features:',features)\n",
    "    img = tf.decode_raw(features['img_raw'],tf.uint8)\n",
    "    img.set_shape([config.img_height * config.img_height])\n",
    "\n",
    "    img = tf.cast(img,tf.float32)*(1./255)\n",
    "    label = tf.cast(features['label'],tf.float32)\n",
    "#     img,label\n",
    "    ###########################################\n",
    "    #下面是shuffle\n",
    "    img_batch,label_batch = tf.train.shuffle_batch([img,label],\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   num_threads=2,\n",
    "                                                   capacity=10000,\n",
    "                                                   min_after_dequeue=5000)\n",
    "\n",
    "    #img_batch,label_batch\n",
    "    \n",
    "    #img_batch, label_batch = get_tfrecord(BATCH_SIZE, config.tfRecord_train, dbg = True)\n",
    "    #############################################\n",
    "    print(type(img))\n",
    "    print(type(label))\n",
    "    print(type(img_batch))\n",
    "    print(type(label_batch))\n",
    "    print(sess.run(serialized_example))\n",
    "#     xs, ys = sess.run([img_batch, label_batch])\n",
    "#     print(type(xs))\n",
    "#     print(type(ys))\n",
    "#     print('ys:',ys)\n",
    "#     reshaped_xs = np.reshape(xs, (BATCH_SIZE,config.img_width,config.img_height,NUM_CHANNELS))\n",
    "    #数据出了点问题，train那个tfrecord0字节，可能有覆写\n",
    "#             _, loss_value, accuracy_value, step = sess.run([train_op, total_loss, accuracy, global_step],\n",
    "#                                                           feed_dict={x:reshaped_xs, y_:ys})\n",
    "#             print('loss:',loss_val)\n",
    "#             if (i + 1) % 2 == 0:\n",
    "#                 print(\"%s : After %d training step(s), loss,accuracy on training batch is %g , %g.\" % (\n",
    "#                 time.strftime('%Y-%m-%d %H:%M:%S'), step, loss_value,accuracy_value))\n",
    "#                 saver.save(sess, os.path.join(config.MODEL_SAVE_PATH, config.MODEL_NAME),global_step=global_step)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# help(tf.train.string_input_producer)\n",
    "# help(tf.train.Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC中是否有必要用bias_add\n",
    "不算batch的话，输出和bias已经是一对一了。\n",
    "conv中要多一个维度，因为二维特征图的存在。\n",
    "FC中两种都可以，CONV也允许,mnist lenet5的例子也没用bias_add，多虑了！\n",
    "\n",
    "但是有一个情况记得是没通过，可能shape特殊或者没处理好。其实反而是用bias_add会报错，batch那一维丢了就不够条件了，可能是一些非batch的手动测试数据，出的问题。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_49:0\", shape=(2, 3), dtype=float64)\n",
      "Tensor(\"Const_50:0\", shape=(2, 2, 2, 3), dtype=float64)\n",
      "[[  1.1   1.2   2.3]\n",
      " [ 11.1  11.2  12.3]]\n",
      "[[  1.1   1.2   2.3]\n",
      " [ 11.1  11.2  12.3]]\n",
      "[[[[  1.1   2.2   3.3]\n",
      "   [  4.1   5.2   6.3]]\n",
      "\n",
      "  [[  7.1   8.2   9.3]\n",
      "   [ 10.1  11.2  12.3]]]\n",
      "\n",
      "\n",
      " [[[ 11.1  12.2  13.3]\n",
      "   [ 14.1  15.2  16.3]]\n",
      "\n",
      "  [[ 17.1  18.2  19.3]\n",
      "   [ 20.1  21.2  22.3]]]]\n",
      "[[[[  1.1   2.2   3.3]\n",
      "   [  4.1   5.2   6.3]]\n",
      "\n",
      "  [[  7.1   8.2   9.3]\n",
      "   [ 10.1  11.2  12.3]]]\n",
      "\n",
      "\n",
      " [[[ 11.1  12.2  13.3]\n",
      "   [ 14.1  15.2  16.3]]\n",
      "\n",
      "  [[ 17.1  18.2  19.3]\n",
      "   [ 20.1  21.2  22.3]]]]\n",
      "[ 1.1  1.2  2.3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#模拟FC输出，batch=2\n",
    "a = tf.constant(np.array([[1.,1.,2.],\n",
    "                         [11.,11.,12.]\n",
    "                         ]))\n",
    "#模拟CONV输出，batch=2，三像素，2个像素点\n",
    "#(2,2,2,3)\n",
    "a2 = tf.constant(np.array([\n",
    "    [[[1.,2.,3.],[4.,5.,6.]],\n",
    "     [[7.,8.,9.],[10.,11.,12.]]],\n",
    "    [[[11.,12.,13.],[14.,15.,16.]],\n",
    "     [[17.,18.,19.],[20.,21.,22.]]]\n",
    "                    ]))\n",
    "#之前可能犯的错误，batch那一维没了，这样用bias_add是不允许的\n",
    "a3 = tf.constant(np.array([1.,1.,2.]))\n",
    "print(a)\n",
    "print(a2)\n",
    "b = tf.constant(np.array([0.1, 0.2, 0.3]))\n",
    "c = tf.nn.bias_add(a,b)\n",
    "d = a+b\n",
    "\n",
    "c2 = tf.nn.bias_add(a2,b)\n",
    "d2 = a2+b\n",
    "\n",
    "# c3 = tf.nn.bias_add(a3,b)\n",
    "d3 = a3+b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(d))\n",
    "    print(sess.run(c2))\n",
    "    print(sess.run(d2))\n",
    "    print(sess.run(d3))\n",
    "#     print(sess.run(c3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_54:0\", shape=(64,), dtype=float32)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#指定数值并指定shape，可以给每一个都初始化为指定数值/\n",
    "a = tf.constant(0.0, dtype = tf.float32, shape=[CONV1_OUTPUT_CHANNEL])\n",
    "print(a)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_3:0\", shape=(7,), dtype=float32)\n",
      "Tensor(\"zeros_4:0\", shape=(7,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#这个默认类型可以不指定\n",
    "a = tf.zeros(shape=[OUTPUT_NODE], dtype=tf.float32)\n",
    "a2 = tf.zeros(shape=[OUTPUT_NODE])\n",
    "print(a)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"test2_2/a:0\", shape=(2,), dtype=float32)\n",
      "[array([ 2.,  2.], dtype=float32), array([ 3.,  3.], dtype=float32), array([ 2.,  2.], dtype=float32), array([ 3.,  3.], dtype=float32), array([ 2.,  2.], dtype=float32), array([ 3.,  3.], dtype=float32)]\n",
      "[ 2.  2.]\n",
      "[ 3.  3.]\n",
      "[ 15.  15.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# help(tf.add_n)\n",
    "with tf.variable_scope('test2'):\n",
    "    a = tf.constant(2.0, dtype = tf.float32, shape=[2],name='a')\n",
    "    a2 = tf.constant(3.0, dtype = tf.float32, shape=[2],name='a2')\n",
    "    tf.add_to_collection('test2', a)\n",
    "    tf.add_to_collection('test2', a2)\n",
    "b = tf.add_n(tf.get_collection('test2'))\n",
    "print(a)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.get_collection('test2')))\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(a2))\n",
    "    print(sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 为了找出BN的update操作为什么到了EMA做测试\n",
    "实测，BN的average操作还在tf.GraphKeys.UPDATE_OPS中\n",
    "另一个例子为什么不行了呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=() dtype=int32_ref>, <tf.Variable 'conv1_w:0' shape=(5, 5, 1, 64) dtype=float32_ref>, <tf.Variable 'conv1_b:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/moving_mean:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/moving_variance:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv2_w:0' shape=(5, 5, 64, 64) dtype=float32_ref>, <tf.Variable 'conv2_b:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/moving_mean:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/moving_variance:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv3_w:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'conv3_b:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'fc1_w:0' shape=(4608, 512) dtype=float32_ref>, <tf.Variable 'fc2_b:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc2_w:0' shape=(512, 512) dtype=float32_ref>, <tf.Variable 'fc2_b_1:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc3_w:0' shape=(512, 7) dtype=float32_ref>, <tf.Variable 'fc3_b:0' shape=(7,) dtype=float32_ref>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'conv1_w/Adam:0' shape=(5, 5, 1, 64) dtype=float32_ref>, <tf.Variable 'conv1_w/Adam_1:0' shape=(5, 5, 1, 64) dtype=float32_ref>, <tf.Variable 'conv1_b/Adam:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv1_b/Adam_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/gamma/Adam:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/gamma/Adam_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/beta/Adam:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn1/beta/Adam_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv2_w/Adam:0' shape=(5, 5, 64, 64) dtype=float32_ref>, <tf.Variable 'conv2_w/Adam_1:0' shape=(5, 5, 64, 64) dtype=float32_ref>, <tf.Variable 'conv2_b/Adam:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv2_b/Adam_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/gamma/Adam:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/gamma/Adam_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/beta/Adam:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn2/beta/Adam_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv3_w/Adam:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'conv3_w/Adam_1:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'conv3_b/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'conv3_b/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/gamma/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/gamma/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/beta/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bn3/beta/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'fc1_w/Adam:0' shape=(4608, 512) dtype=float32_ref>, <tf.Variable 'fc1_w/Adam_1:0' shape=(4608, 512) dtype=float32_ref>, <tf.Variable 'fc2_b/Adam:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc2_b/Adam_1:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc2_w/Adam:0' shape=(512, 512) dtype=float32_ref>, <tf.Variable 'fc2_w/Adam_1:0' shape=(512, 512) dtype=float32_ref>, <tf.Variable 'fc2_b_1/Adam:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc2_b_1/Adam_1:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'fc3_w/Adam:0' shape=(512, 7) dtype=float32_ref>, <tf.Variable 'fc3_w/Adam_1:0' shape=(512, 7) dtype=float32_ref>, <tf.Variable 'fc3_b/Adam:0' shape=(7,) dtype=float32_ref>, <tf.Variable 'fc3_b/Adam_1:0' shape=(7,) dtype=float32_ref>]\n",
      "update_ops: [<tf.Tensor 'bn1/AssignMovingAvg:0' shape=(64,) dtype=float32_ref>, <tf.Tensor 'bn1/AssignMovingAvg_1:0' shape=(64,) dtype=float32_ref>, <tf.Tensor 'bn2/AssignMovingAvg:0' shape=(64,) dtype=float32_ref>, <tf.Tensor 'bn2/AssignMovingAvg_1:0' shape=(64,) dtype=float32_ref>, <tf.Tensor 'bn3/AssignMovingAvg:0' shape=(128,) dtype=float32_ref>, <tf.Tensor 'bn3/AssignMovingAvg_1:0' shape=(128,) dtype=float32_ref>]\n",
      "\n",
      "ema: <tensorflow.python.training.moving_averages.ExponentialMovingAverage object at 0x7fcafe093ef0>\n",
      "\n",
      "ema_op: name: \"ema\"\n",
      "op: \"NoOp\"\n",
      "input: \"^ema/AssignMovingAvg\"\n",
      "input: \"^ema/AssignMovingAvg_1\"\n",
      "input: \"^ema/AssignMovingAvg_2\"\n",
      "input: \"^ema/AssignMovingAvg_3\"\n",
      "input: \"^ema/AssignMovingAvg_4\"\n",
      "input: \"^ema/AssignMovingAvg_5\"\n",
      "input: \"^ema/AssignMovingAvg_6\"\n",
      "input: \"^ema/AssignMovingAvg_7\"\n",
      "input: \"^ema/AssignMovingAvg_8\"\n",
      "input: \"^ema/AssignMovingAvg_9\"\n",
      "input: \"^ema/AssignMovingAvg_10\"\n",
      "input: \"^ema/AssignMovingAvg_11\"\n",
      "input: \"^ema/AssignMovingAvg_12\"\n",
      "input: \"^ema/AssignMovingAvg_13\"\n",
      "input: \"^ema/AssignMovingAvg_14\"\n",
      "input: \"^ema/AssignMovingAvg_15\"\n",
      "input: \"^ema/AssignMovingAvg_16\"\n",
      "input: \"^ema/AssignMovingAvg_17\"\n",
      "\n",
      "type: <class 'tensorflow.python.framework.ops.Operation'> <class 'list'>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The name 'update_ops' refers to an Operation not in the graph.\"",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-51d77f67f0e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#先转换类型再计算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mbackward2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-51d77f67f0e2>\u001b[0m in \u001b[0;36mbackward2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m#不行，tensor和list的组合如何写？分开依赖！或者别用get_collection！保持ops类型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#     with tf.control_dependencies([train_step, update_ops]):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUPDATE_OPS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcontrol_dependencies\u001b[0;34m(control_inputs)\u001b[0m\n\u001b[1;32m   4302\u001b[0m   \"\"\"\n\u001b[1;32m   4303\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4304\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrol_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4305\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4306\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_NullContextmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcontrol_dependencies\u001b[0;34m(self, control_inputs)\u001b[0m\n\u001b[1;32m   4015\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexedSlices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4016\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4017\u001b[0;31m       \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4018\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3034\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3035\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3094\u001b[0m           raise KeyError(\"The name %s refers to an Operation not in the \"\n\u001b[0;32m-> 3095\u001b[0;31m                          \"graph.\" % repr(name))\n\u001b[0m\u001b[1;32m   3096\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The name 'update_ops' refers to an Operation not in the graph.\""
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import fer_config as config\n",
    "import fer_generator\n",
    "import fer_forward\n",
    "import time\n",
    "NUM_CHANNELS = 1\n",
    "OUTPUT_NODE = 7\n",
    "BATCH_SIZE = 4#128对于虚拟机CPU来说，可能有点大。\n",
    "LEARNING_RATE_BASE = 0.0005\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "STEPS = 1000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "train_num_examples = 28709\n",
    "def backward2():\n",
    "    global_step = tf.Variable(0, trainable = False)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        train_num_examples / BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [BATCH_SIZE, config.img_width, config.img_height, NUM_CHANNELS])\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE])\n",
    "    \n",
    "    y = fer_forward.forward(x,True)\n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))\n",
    "    cem = tf.reduce_mean(ce,name='cross_entropy_reduce_mean')\n",
    "    total_loss = tf.add(cem, tf.add_n(tf.get_collection('regularization_losses')), name='total_loss')\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(total_loss)#很多变量Adam有两次操作，因为正则化\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    print(tf.global_variables())\n",
    "    print('update_ops:',update_ops)\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step,name='ema')\n",
    "    print('\\nema:',ema)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    print('\\nema_op:',ema_op)\n",
    "    \n",
    "    print('type:',type(train_step), type(update_ops))\n",
    "    \n",
    "    if 1:\n",
    "#     with tf.control_dependencies([train_step, update_ops]):#不行，tensor和list的组合如何写？分开依赖！或者别用get_collection！保持ops类型\n",
    "        with tf.control_dependencies([train_step, tf.GraphKeys.UPDATE_OPS]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "    else:\n",
    "        with tf.control_dependencies( update_ops):\n",
    "            train_op = tf.no_op(name='update_ops')\n",
    "        with tf.control_dependencies([train_step, train_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name='accuracy')#先转换类型再计算\n",
    "    \n",
    "backward2() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# help(tf.nn.l2_loss)#output = sum(t ** 2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# help(tf.nn.conv2d)#先kernel，再inout\n",
    "# help(tf.nn.lrn)\n",
    "# help(tf.train.Saver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 练习：truncated数据初始化，观察数据规律\n",
    "tf_demo/api_demo/truncated_normal.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:/MachineLearning/model_saved/fer2013'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fer_config as config\n",
    "config.MODEL_SAVE_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL\n",
    "PIL.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function histogram in module tensorflow.python.summary.summary:\n",
      "\n",
      "histogram(name, values, collections=None, family=None)\n",
      "    Outputs a `Summary` protocol buffer with a histogram.\n",
      "    \n",
      "    Adding a histogram summary makes it possible to visualize your data's\n",
      "    distribution in TensorBoard. You can see a detailed explanation of the\n",
      "    TensorBoard histogram dashboard\n",
      "    [here](https://www.tensorflow.org/get_started/tensorboard_histograms).\n",
      "    \n",
      "    The generated\n",
      "    [`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)\n",
      "    has one summary value containing a histogram for `values`.\n",
      "    \n",
      "    This op reports an `InvalidArgument` error if any value is not finite.\n",
      "    \n",
      "    Args:\n",
      "      name: A name for the generated node. Will also serve as a series name in\n",
      "        TensorBoard.\n",
      "      values: A real numeric `Tensor`. Any shape. Values to use to\n",
      "        build the histogram.\n",
      "      collections: Optional list of graph collections keys. The new summary op is\n",
      "        added to these collections. Defaults to `[GraphKeys.SUMMARIES]`.\n",
      "      family: Optional; if provided, used as the prefix of the summary tag name,\n",
      "        which controls the tab name used for display on Tensorboard.\n",
      "    \n",
      "    Returns:\n",
      "      A scalar `Tensor` of type `string`. The serialized `Summary` protocol\n",
      "      buffer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "help(tf.summary.histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
